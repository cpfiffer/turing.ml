<p><a id="Guide"></a></p>

<p><a id="Guide-1"></a></p>

<h1 id="guide">Guide</h1>

<p><a id="Basics"></a></p>

<p><a id="Basics-1"></a></p>

<h2 id="basics">Basics</h2>

<p><a id="Introduction"></a></p>

<p><a id="Introduction-1"></a></p>

<h3 id="introduction">Introduction</h3>

<p>A probabilistic program is Julia code wrapped in a <code class="language-plaintext highlighter-rouge">@model</code> macro. It can use arbitrary Julia code, but to ensure correctness of inference it should not have external effects or modify global state. Stack-allocated variables are safe, but mutable heap-allocated objects may lead to subtle bugs when using task copying. To help avoid those we provide a Turing-safe datatype <code class="language-plaintext highlighter-rouge">TArray</code> that can be used to create mutable arrays in Turing programs.</p>

<p>To specify distributions of random variables, Turing programs should use the <code class="language-plaintext highlighter-rouge">~</code> notation:</p>

<p><code class="language-plaintext highlighter-rouge">x ~ distr</code> where <code class="language-plaintext highlighter-rouge">x</code> is a symbol and <code class="language-plaintext highlighter-rouge">distr</code> is a distribution. If <code class="language-plaintext highlighter-rouge">x</code> is undefined in the model function, inside the probabilistic program, this puts a random variable named <code class="language-plaintext highlighter-rouge">x</code>, distributed according to <code class="language-plaintext highlighter-rouge">distr</code>, in the current scope. <code class="language-plaintext highlighter-rouge">distr</code> can be a value of any type that implements <code class="language-plaintext highlighter-rouge">rand(distr)</code>, which samples a value from the distribution <code class="language-plaintext highlighter-rouge">distr</code>. If <code class="language-plaintext highlighter-rouge">x</code> is defined, this is used for conditioning in a style similar to <a href="https://probprog.github.io/anglican/index.html">Anglican</a> (another PPL). In this case, <code class="language-plaintext highlighter-rouge">x</code> is an observed value, assumed to have been drawn from the distribution <code class="language-plaintext highlighter-rouge">distr</code>. The likelihood is computed using <code class="language-plaintext highlighter-rouge">logpdf(distr,y)</code>. The observe statements should be arranged so that every possible run traverses all of them in exactly the same order. This is equivalent to demanding that they are not placed inside stochastic control flow.</p>

<p>Available inference methods include Importance Sampling (IS), Sequential Monte Carlo (SMC), Particle Gibbs (PG), Hamiltonian Monte Carlo (HMC), Hamiltonian Monte Carlo with Dual Averaging (HMCDA) and The No-U-Turn Sampler (NUTS).</p>

<p><a id="Simple-Gaussian-Demo"></a></p>

<p><a id="Simple-Gaussian-Demo-1"></a></p>

<h3 id="simple-gaussian-demo">Simple Gaussian Demo</h3>

<p>Below is a simple Gaussian demo illustrate the basic usage of Turing.jl.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Import packages.</span>
<span class="k">using</span> <span class="n">Turing</span>
<span class="k">using</span> <span class="n">StatsPlots</span>

<span class="c"># Define a simple Normal model with unknown mean and variance.</span>
<span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">x</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">y</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Note: As a sanity check, the expectation of <code class="language-plaintext highlighter-rouge">s</code> is 49/24 (2.04166666…) and the expectation of <code class="language-plaintext highlighter-rouge">m</code> is 7/6 (1.16666666…).</p>

<p>We can perform inference by using the <code class="language-plaintext highlighter-rouge">sample</code> function, the first argument of which is our probabalistic program and the second of which is a sampler. More information on each sampler is located in the <a href="/v0.14/docs/library">API</a>.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#  Run sampler, collect results.</span>
<span class="n">c1</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">),</span> <span class="n">SMC</span><span class="x">(),</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">c2</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">),</span> <span class="n">PG</span><span class="x">(</span><span class="mi">10</span><span class="x">),</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">c3</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">),</span> <span class="n">HMC</span><span class="x">(</span><span class="mf">0.1</span><span class="x">,</span> <span class="mi">5</span><span class="x">),</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">c4</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">),</span> <span class="n">Gibbs</span><span class="x">(</span><span class="n">PG</span><span class="x">(</span><span class="mi">10</span><span class="x">,</span> <span class="o">:</span><span class="n">m</span><span class="x">),</span> <span class="n">HMC</span><span class="x">(</span><span class="mf">0.1</span><span class="x">,</span> <span class="mi">5</span><span class="x">,</span> <span class="o">:</span><span class="n">s</span><span class="x">)),</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">c5</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">),</span> <span class="n">HMCDA</span><span class="x">(</span><span class="mf">0.15</span><span class="x">,</span> <span class="mf">0.65</span><span class="x">),</span> <span class="mi">1000</span><span class="x">)</span>
<span class="n">c6</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">),</span> <span class="n">NUTS</span><span class="x">(</span><span class="mf">0.65</span><span class="x">),</span> <span class="mi">1000</span><span class="x">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">MCMCChains</code> module (which is re-exported by Turing) provides plotting tools for the <code class="language-plaintext highlighter-rouge">Chain</code> objects returned by a <code class="language-plaintext highlighter-rouge">sample</code> function. See the <a href="https://github.com/TuringLang/MCMCChains.jl">MCMCChains</a> repository for more information on the suite of tools available for diagnosing MCMC chains.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Summarise results</span>
<span class="n">describe</span><span class="x">(</span><span class="n">c3</span><span class="x">)</span>

<span class="c"># Plot results</span>
<span class="n">plot</span><span class="x">(</span><span class="n">c3</span><span class="x">)</span>
<span class="n">savefig</span><span class="x">(</span><span class="s">"gdemo-plot.png"</span><span class="x">)</span>
</code></pre></div></div>

<p>The arguments for each sampler are:</p>

<ul>
  <li>SMC: number of particles.</li>
  <li>PG: number of particles, number of iterations.</li>
  <li>HMC: leapfrog step size, leapfrog step numbers.</li>
  <li>Gibbs: component sampler 1, component sampler 2, …</li>
  <li>HMCDA: total leapfrog length, target accept ratio.</li>
  <li>NUTS: number of adaptation steps (optional), target accept ratio.</li>
</ul>

<p>For detailed information on the samplers, please review Turing.jl’s <a href="/v0.14/docs/library">API</a> documentation.</p>

<p><a id="Modelling-Syntax-Explained"></a></p>

<p><a id="Modelling-Syntax-Explained-1"></a></p>

<h3 id="modelling-syntax-explained">Modelling Syntax Explained</h3>

<p>Using this syntax, a probabilistic model is defined in Turing. The model function generated by Turing can then be used to condition the model onto data. Subsequently, the sample function can be used to generate samples from the posterior distribution.</p>

<p>In the following example, the defined model is conditioned to the date (arg<em>1 = 1, arg</em>2 = 2) by passing (1, 2) to the model function.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="k">function</span><span class="nf"> model_name</span><span class="x">(</span><span class="n">arg_1</span><span class="x">,</span> <span class="n">arg_2</span><span class="x">)</span>
  <span class="o">...</span>
<span class="k">end</span>
</code></pre></div></div>

<p>The conditioned model can then be passed onto the sample function to run posterior inference.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_func</span> <span class="o">=</span> <span class="n">model_name</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span> <span class="mi">2</span><span class="x">)</span>
<span class="n">chn</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model_func</span><span class="x">,</span> <span class="n">HMC</span><span class="x">(</span><span class="o">..</span><span class="x">))</span> <span class="c"># Perform inference by sampling using HMC.</span>
</code></pre></div></div>

<p>The returned chain contains samples of the variables in the model.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">var_1</span> <span class="o">=</span> <span class="n">mean</span><span class="x">(</span><span class="n">chn</span><span class="x">[</span><span class="o">:</span><span class="n">var_1</span><span class="x">])</span> <span class="c"># Taking the mean of a variable named var_1.</span>
</code></pre></div></div>

<p>The key (<code class="language-plaintext highlighter-rouge">:var_1</code>) can be a <code class="language-plaintext highlighter-rouge">Symbol</code> or a <code class="language-plaintext highlighter-rouge">String</code>. For example, to fetch <code class="language-plaintext highlighter-rouge">x[1]</code>, one can use <code class="language-plaintext highlighter-rouge">chn[Symbol("x[1]")</code> or <code class="language-plaintext highlighter-rouge">chn["x[1]"]</code>. If you want to retrieve all parameters associated with a specific symbol, you can use <code class="language-plaintext highlighter-rouge">group</code>. As an example, if you have the parameters <code class="language-plaintext highlighter-rouge">"x[1]"</code>, <code class="language-plaintext highlighter-rouge">"x[2]"</code>, and <code class="language-plaintext highlighter-rouge">"x[3]"</code>, calling <code class="language-plaintext highlighter-rouge">group(chn, :x)</code> or <code class="language-plaintext highlighter-rouge">group(chn, "x")</code> will return a new chain with only <code class="language-plaintext highlighter-rouge">"x[1]"</code>, <code class="language-plaintext highlighter-rouge">"x[2]"</code>, and <code class="language-plaintext highlighter-rouge">"x[3]"</code>.</p>

<p>Turing does not have a declarative form. More generally, the order in which you place the lines of a <code class="language-plaintext highlighter-rouge">@model</code> macro matters. For example, the following example works:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define a simple Normal model with unknown mean and variance.</span>
<span class="nd">@model</span> <span class="k">function</span><span class="nf"> model_function</span><span class="x">(</span><span class="n">y</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">Poisson</span><span class="x">(</span><span class="mi">1</span><span class="x">)</span>
    <span class="n">y</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">s</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">end</span>

<span class="n">sample</span><span class="x">(</span><span class="n">model_function</span><span class="x">(</span><span class="mi">10</span><span class="x">),</span> <span class="n">SMC</span><span class="x">(),</span> <span class="mi">100</span><span class="x">)</span>
</code></pre></div></div>

<p>But if we switch the <code class="language-plaintext highlighter-rouge">s ~ Poisson(1)</code> and <code class="language-plaintext highlighter-rouge">y ~ Normal(s, 1)</code> lines, the model will no longer sample correctly:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define a simple Normal model with unknown mean and variance.</span>
<span class="nd">@model</span> <span class="k">function</span><span class="nf"> model_function</span><span class="x">(</span><span class="n">y</span><span class="x">)</span>
    <span class="n">y</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">s</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">Poisson</span><span class="x">(</span><span class="mi">1</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">end</span>

<span class="n">sample</span><span class="x">(</span><span class="n">model_function</span><span class="x">(</span><span class="mi">10</span><span class="x">),</span> <span class="n">SMC</span><span class="x">(),</span> <span class="mi">100</span><span class="x">)</span>
</code></pre></div></div>

<p><a id="Sampling-Multiple-Chains"></a></p>

<p><a id="Sampling-Multiple-Chains-1"></a></p>

<h3 id="sampling-multiple-chains">Sampling Multiple Chains</h3>

<p>Turing supports distributed and threaded parallel sampling. To do so, call <code class="language-plaintext highlighter-rouge">sample(model, sampler, parallel_type, n, n_chains)</code>, where <code class="language-plaintext highlighter-rouge">parallel_type</code> can be either <code class="language-plaintext highlighter-rouge">MCMCThreads()</code> or <code class="language-plaintext highlighter-rouge">MCMCDistributed()</code> for thread and parallel sampling, respectively.</p>

<p>Having multiple chains in the same object is valuable for evaluating convergence. Some diagnostic functions like <code class="language-plaintext highlighter-rouge">gelmandiag</code> require multiple chains.</p>

<p>If you do not want parallelism or are on an older version Julia, you can sample multiple chains with the <code class="language-plaintext highlighter-rouge">mapreduce</code> function:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace num_chains below with however many chains you wish to sample.</span>
<span class="n">chains</span> <span class="o">=</span> <span class="n">mapreduce</span><span class="x">(</span><span class="n">c</span> <span class="o">-&gt;</span> <span class="n">sample</span><span class="x">(</span><span class="n">model_fun</span><span class="x">,</span> <span class="n">sampler</span><span class="x">,</span> <span class="mi">1000</span><span class="x">),</span> <span class="n">chainscat</span><span class="x">,</span> <span class="mi">1</span><span class="o">:</span><span class="n">num_chains</span><span class="x">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">chains</code> variable now contains a <code class="language-plaintext highlighter-rouge">Chains</code> object which can be indexed by chain. To pull out the first chain from the <code class="language-plaintext highlighter-rouge">chains</code> object, use <code class="language-plaintext highlighter-rouge">chains[:,:,1]</code>. The method is the same if you use either of the below parallel sampling methods.</p>

<p><a id="Multithreaded-sampling"></a></p>

<p><a id="Multithreaded-sampling-1"></a></p>

<h4 id="multithreaded-sampling">Multithreaded sampling</h4>

<p>If you wish to perform multithreaded sampling and are running Julia 1.3 or greater, you can call <code class="language-plaintext highlighter-rouge">sample</code> with the following signature:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span>

<span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">eachindex</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">([</span><span class="mf">1.5</span><span class="x">,</span> <span class="mf">2.0</span><span class="x">])</span>

<span class="c"># Sample four chains using multiple threads, each with 1000 samples.</span>
<span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">NUTS</span><span class="x">(),</span> <span class="n">MCMCThreads</span><span class="x">(),</span> <span class="mi">1000</span><span class="x">,</span> <span class="mi">4</span><span class="x">)</span>
</code></pre></div></div>

<p>Be aware that Turing cannot add threads for you – you must have started your Julia instance with multiple threads to experience any kind of parallelism. See the <a href="https://docs.julialang.org/en/v1/manual/parallel-computing/#man-multithreading-1">Julia documentation</a> for details on how to achieve this.</p>

<p><a id="Distributed-sampling"></a></p>

<p><a id="Distributed-sampling-1"></a></p>

<h4 id="distributed-sampling">Distributed sampling</h4>

<p>To perform distributed sampling (using multiple processes), you must first import <code class="language-plaintext highlighter-rouge">Distributed</code>.</p>

<p>Process parallel sampling can be done like so:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Load Distributed to add processes and the @everywhere macro.</span>
<span class="k">using</span> <span class="n">Distributed</span> 

<span class="c"># Load Turing.</span>
<span class="k">using</span> <span class="n">Turing</span>

<span class="c"># Add four processes to use for sampling. </span>
<span class="n">addprocs</span><span class="x">(</span><span class="mi">4</span><span class="x">)</span>

<span class="c"># Initialize everything on all the processes.</span>
<span class="c"># Note: Make sure to do this after you've already loaded Turing,</span>
<span class="c">#       so each process does not have to precompile. </span>
<span class="c">#       Parallel sampling may fail silently if you do not do this.</span>
<span class="nd">@everywhere</span> <span class="k">using</span> <span class="n">Turing</span>

<span class="c"># Define a model on all processes.</span>
<span class="nd">@everywhere</span> <span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">eachindex</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="c"># Declare the model instance everywhere.</span>
<span class="nd">@everywhere</span> <span class="n">model</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">([</span><span class="mf">1.5</span><span class="x">,</span> <span class="mf">2.0</span><span class="x">])</span>

<span class="c"># Sample four chains using multiple processes, each with 1000 samples.</span>
<span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">NUTS</span><span class="x">(),</span> <span class="n">MCMCDistributed</span><span class="x">(),</span> <span class="mi">1000</span><span class="x">,</span> <span class="mi">4</span><span class="x">)</span>
</code></pre></div></div>

<p><a id="Sampling-from-an-Unconditional-Distribution-(The-Prior)"></a></p>

<p><a id="Sampling-from-an-Unconditional-Distribution-(The-Prior)-1"></a></p>

<h3 id="sampling-from-an-unconditional-distribution-the-prior">Sampling from an Unconditional Distribution (The Prior)</h3>

<p>Turing allows you to sample from a declared model’s prior. If you wish to draw a chain from the prior to inspect your prior distributions, you can simply run</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">Prior</span><span class="x">(),</span> <span class="n">n_samples</span><span class="x">)</span>
</code></pre></div></div>

<p>You can also run your model (as if it were a function) from the prior distribution, by calling the model without specifying inputs or a sampler. In the below example, we specify a <code class="language-plaintext highlighter-rouge">gdemo</code> model which returns two variables, <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>. The model includes <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> as arguments, but calling the function without passing in <code class="language-plaintext highlighter-rouge">x</code> or <code class="language-plaintext highlighter-rouge">y</code> means that Turing’s compiler will assume they are missing values to draw from the relevant distribution. The <code class="language-plaintext highlighter-rouge">return</code> statement is necessary to retrieve the sampled <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> values.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">x</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">y</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">return</span> <span class="n">x</span><span class="x">,</span> <span class="n">y</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Assign the function with <code class="language-plaintext highlighter-rouge">missing</code> inputs to a variable, and Turing will produce a sample from the prior distribution.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Samples from p(x,y)</span>
<span class="n">g_prior_sample</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">(</span><span class="nb">missing</span><span class="x">,</span> <span class="nb">missing</span><span class="x">)</span>
<span class="n">g_prior_sample</span><span class="x">()</span>
</code></pre></div></div>

<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0.685690547873451, -1.1972706455914328)
</code></pre></div></div>

<p><a id="Sampling-from-a-Conditional-Distribution-(The-Posterior)"></a></p>

<p><a id="Sampling-from-a-Conditional-Distribution-(The-Posterior)-1"></a></p>

<h3 id="sampling-from-a-conditional-distribution-the-posterior">Sampling from a Conditional Distribution (The Posterior)</h3>

<p><a id="Treating-observations-as-random-variables"></a></p>

<p><a id="Treating-observations-as-random-variables-1"></a></p>

<h4 id="treating-observations-as-random-variables">Treating observations as random variables</h4>

<p>Inputs to the model that have a value <code class="language-plaintext highlighter-rouge">missing</code> are treated as parameters, aka random variables, to be estimated/sampled. This can be useful if you want to simulate draws for that parameter, or if you are sampling from a conditional distribution. Turing supports the following syntax:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="o">::</span><span class="kt">Type</span><span class="x">{</span><span class="n">T</span><span class="x">}</span> <span class="o">=</span> <span class="kt">Float64</span><span class="x">)</span> <span class="k">where</span> <span class="x">{</span><span class="n">T</span><span class="x">}</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">===</span> <span class="nb">missing</span>
        <span class="c"># Initialize `x` if missing</span>
        <span class="n">x</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="n">T</span><span class="x">}(</span><span class="nb">undef</span><span class="x">,</span> <span class="mi">2</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">eachindex</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="c"># Construct a model with x = missing</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">(</span><span class="nb">missing</span><span class="x">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">HMC</span><span class="x">(</span><span class="mf">0.01</span><span class="x">,</span> <span class="mi">5</span><span class="x">),</span> <span class="mi">500</span><span class="x">)</span>
</code></pre></div></div>

<p>Note the need to initialize <code class="language-plaintext highlighter-rouge">x</code> when missing since we are iterating over its elements later in the model. The generated values for <code class="language-plaintext highlighter-rouge">x</code> can be extracted from the <code class="language-plaintext highlighter-rouge">Chains</code> object using <code class="language-plaintext highlighter-rouge">c[:x]</code>.</p>

<p>Turing also supports mixed <code class="language-plaintext highlighter-rouge">missing</code> and non-<code class="language-plaintext highlighter-rouge">missing</code> values in <code class="language-plaintext highlighter-rouge">x</code>, where the missing ones will be treated as random variables to be sampled while the others get treated as observations. For example:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">eachindex</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="c"># x[1] is a parameter, but x[2] is an observation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">([</span><span class="nb">missing</span><span class="x">,</span> <span class="mf">2.4</span><span class="x">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">HMC</span><span class="x">(</span><span class="mf">0.01</span><span class="x">,</span> <span class="mi">5</span><span class="x">),</span> <span class="mi">500</span><span class="x">)</span>
</code></pre></div></div>

<p><a id="Default-Values"></a></p>

<p><a id="Default-Values-1"></a></p>

<h4 id="default-values">Default Values</h4>

<p>Arguments to Turing models can have default values much like how default values work in normal Julia functions. For instance, the following will assign <code class="language-plaintext highlighter-rouge">missing</code> to <code class="language-plaintext highlighter-rouge">x</code> and treat it as a random variable. If the default value is not <code class="language-plaintext highlighter-rouge">missing</code>, <code class="language-plaintext highlighter-rouge">x</code> will be assigned that value and will be treated as an observation instead.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Turing</span>

<span class="nd">@model</span> <span class="k">function</span><span class="nf"> generative</span><span class="x">(</span><span class="n">x</span> <span class="o">=</span> <span class="nb">missing</span><span class="x">,</span> <span class="o">::</span><span class="kt">Type</span><span class="x">{</span><span class="n">T</span><span class="x">}</span> <span class="o">=</span> <span class="kt">Float64</span><span class="x">)</span> <span class="k">where</span> <span class="x">{</span><span class="n">T</span> <span class="o">&lt;:</span> <span class="kt">Real</span><span class="x">}</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">===</span> <span class="nb">missing</span>
        <span class="c"># Initialize x when missing</span>
        <span class="n">x</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="n">T</span><span class="x">}(</span><span class="nb">undef</span><span class="x">,</span> <span class="mi">10</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">s</span><span class="x">,</span> <span class="n">m</span>
<span class="k">end</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">generative</span><span class="x">()</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">HMC</span><span class="x">(</span><span class="mf">0.01</span><span class="x">,</span> <span class="mi">5</span><span class="x">),</span> <span class="mi">1000</span><span class="x">)</span>
</code></pre></div></div>

<p><a id="Access-Values-inside-Chain"></a></p>

<p><a id="Access-Values-inside-Chain-1"></a></p>

<h4 id="access-values-inside-chain">Access Values inside Chain</h4>

<p>You can access the values inside a chain several ways:</p>

<ol>
  <li>Turn them into a <code class="language-plaintext highlighter-rouge">DataFrame</code> object</li>
  <li>Use their raw <code class="language-plaintext highlighter-rouge">AxisArray</code> form</li>
  <li>Create a three-dimensional <code class="language-plaintext highlighter-rouge">Array</code> object</li>
</ol>

<p>For example, let <code class="language-plaintext highlighter-rouge">c</code> be a <code class="language-plaintext highlighter-rouge">Chain</code>:     1. <code class="language-plaintext highlighter-rouge">DataFrame(c)</code> converts <code class="language-plaintext highlighter-rouge">c</code> to a <code class="language-plaintext highlighter-rouge">DataFrame</code>,     2. <code class="language-plaintext highlighter-rouge">c.value</code> retrieves the values inside <code class="language-plaintext highlighter-rouge">c</code> as an <code class="language-plaintext highlighter-rouge">AxisArray</code>, and     3. <code class="language-plaintext highlighter-rouge">c.value.data</code> retrieves the values inside <code class="language-plaintext highlighter-rouge">c</code> as a 3D <code class="language-plaintext highlighter-rouge">Array</code>.</p>

<p><a id="Variable-Types-and-Type-Parameters"></a></p>

<p><a id="Variable-Types-and-Type-Parameters-1"></a></p>

<h4 id="variable-types-and-type-parameters">Variable Types and Type Parameters</h4>

<p>The element type of a vector (or matrix) of random variables should match the <code class="language-plaintext highlighter-rouge">eltype</code> of the its prior distribution, <code class="language-plaintext highlighter-rouge">&lt;: Integer</code> for discrete distributions and <code class="language-plaintext highlighter-rouge">&lt;: AbstractFloat</code> for continuous distributions. Moreover, if the continuous random variable is to be sampled using a Hamiltonian sampler, the vector’s element type needs to either be:     1. <code class="language-plaintext highlighter-rouge">Real</code> to enable auto-differentiation through the model which uses special number types that are sub-types of <code class="language-plaintext highlighter-rouge">Real</code>, or     2. Some type parameter <code class="language-plaintext highlighter-rouge">T</code> defined in the model header using the type parameter syntax, e.g. <code class="language-plaintext highlighter-rouge">gdemo(x, ::Type{T} = Float64) where {T} = begin</code>. Similarly, when using a particle sampler, the Julia variable used should either be:     1. A <code class="language-plaintext highlighter-rouge">TArray</code>, or     2. An instance of some type parameter <code class="language-plaintext highlighter-rouge">T</code> defined in the model header using the type parameter syntax, e.g. <code class="language-plaintext highlighter-rouge">gdemo(x, ::Type{T} = Vector{Float64}) where {T} = begin</code>.</p>

<p><a id="Querying-Probabilities-from-Model-or-Chain"></a></p>

<p><a id="Querying-Probabilities-from-Model-or-Chain-1"></a></p>

<h3 id="querying-probabilities-from-model-or-chain">Querying Probabilities from Model or Chain</h3>

<p>Consider the following <code class="language-plaintext highlighter-rouge">gdemo</code> model:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">x</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">y</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
<span class="k">end</span>
</code></pre></div></div>

<p>The following are examples of valid queries of the <code class="language-plaintext highlighter-rouge">Turing</code> model or chain:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">prob"x = 1.0, y = 1.0 | model = gdemo, s = 1.0, m = 1.0"</code> calculates the likelihood of <code class="language-plaintext highlighter-rouge">x = 1</code> and <code class="language-plaintext highlighter-rouge">y = 1</code> given <code class="language-plaintext highlighter-rouge">s = 1</code> and <code class="language-plaintext highlighter-rouge">m = 1</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">prob"s = 1.0, m = 1.0 | model = gdemo, x = nothing, y = nothing"</code> calculates the joint probability of <code class="language-plaintext highlighter-rouge">s = 1</code> and <code class="language-plaintext highlighter-rouge">m = 1</code> ignoring <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>. <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> are ignored so they can be optionally dropped from the RHS of <code class="language-plaintext highlighter-rouge">|</code>, but it is recommended to define them.</li>
  <li><code class="language-plaintext highlighter-rouge">prob"s = 1.0, m = 1.0, x = 1.0 | model = gdemo, y = nothing"</code> calculates the joint probability of <code class="language-plaintext highlighter-rouge">s = 1</code>, <code class="language-plaintext highlighter-rouge">m = 1</code> and <code class="language-plaintext highlighter-rouge">x = 1</code> ignoring <code class="language-plaintext highlighter-rouge">y</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">prob"s = 1.0, m = 1.0, x = 1.0, y = 1.0 | model = gdemo"</code> calculates the joint probability of all the variables.</li>
  <li>After the MCMC sampling, given a <code class="language-plaintext highlighter-rouge">chain</code>, <code class="language-plaintext highlighter-rouge">prob"x = 1.0, y = 1.0 | chain = chain, model = gdemo"</code> calculates the element-wise likelihood of <code class="language-plaintext highlighter-rouge">x = 1.0</code> and <code class="language-plaintext highlighter-rouge">y = 1.0</code> for each sample in <code class="language-plaintext highlighter-rouge">chain</code>.</li>
  <li>If <code class="language-plaintext highlighter-rouge">save_state=true</code> was used during sampling (i.e., <code class="language-plaintext highlighter-rouge">sample(model, sampler, N; save_state=true)</code>), you can simply do <code class="language-plaintext highlighter-rouge">prob"x = 1.0, y = 1.0 | chain = chain"</code>.</li>
</ul>

<p>In all the above cases, <code class="language-plaintext highlighter-rouge">logprob</code> can be used instead of <code class="language-plaintext highlighter-rouge">prob</code> to calculate the log probabilities instead.</p>

<p><a id="Maximum-likelihood-and-maximum-a-posterior-estimates"></a></p>

<p><a id="Maximum-likelihood-and-maximum-a-posterior-estimates-1"></a></p>

<h3 id="maximum-likelihood-and-maximum-a-posterior-estimates">Maximum likelihood and maximum a posterior estimates</h3>

<p>Turing provides support for two mode estimation techniques, <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> (MLE) and <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">maximum a posterior</a> (MAP) estimation. Optimization is performed by the <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> package. Mode estimation is currently a optional tool, and will not be available to you unless you have manually installed Optim and loaded the package with a <code class="language-plaintext highlighter-rouge">using</code> statement. To install Optim, run <code class="language-plaintext highlighter-rouge">import Pkg; Pkg.add("Optim")</code>.</p>

<p>Mode estimation only works when all model parameters are continuous – discrete parameters cannot be estimated with MLE/MAP as of yet.</p>

<p>To understand how mode estimation works, let us first load Turing and Optim to enable mode estimation, and then declare a model:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Note that loading Optim explicitly is required for mode estimation to function,</span>
<span class="c"># as Turing does not load the opimization suite unless Optim is loaded as well.</span>
<span class="k">using</span> <span class="n">Turing</span>
<span class="k">using</span> <span class="n">Optim</span>

<span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">eachindex</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
        <span class="n">x</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Once the model is defined, we can construct a model instance as we normally would:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create some data to pass to the model.</span>
<span class="n">data</span> <span class="o">=</span> <span class="x">[</span><span class="mf">1.5</span><span class="x">,</span> <span class="mf">2.0</span><span class="x">]</span>

<span class="c"># Instantiate the gdemo model with our data.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">(</span><span class="n">data</span><span class="x">)</span>
</code></pre></div></div>

<p>Mode estimation is typically quick and easy at this point. Turing extends the function <code class="language-plaintext highlighter-rouge">Optim.optimize</code> and accepts the structs <code class="language-plaintext highlighter-rouge">MLE()</code> or <code class="language-plaintext highlighter-rouge">MAP()</code>, which inform Turing whether to provide an MLE or MAP estimate, respectively. By default, the <a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/">LBFGS optimizer</a> is used, though this can be changed. Basic usage is:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Generate a MLE estimate.</span>
<span class="n">mle_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MLE</span><span class="x">())</span>

<span class="c"># Generate a MAP estimate.</span>
<span class="n">map_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MAP</span><span class="x">())</span>
</code></pre></div></div>

<p>If you wish to change to a different optimizer, such as <code class="language-plaintext highlighter-rouge">NelderMead</code>, simply place your optimizer in the third argument slot:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use NelderMead</span>
<span class="n">mle_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MLE</span><span class="x">(),</span> <span class="n">NelderMead</span><span class="x">())</span>

<span class="c"># Use SimulatedAnnealing</span>
<span class="n">mle_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MLE</span><span class="x">(),</span> <span class="n">SimulatedAnnealing</span><span class="x">())</span>

<span class="c"># Use ParticleSwarm</span>
<span class="n">mle_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MLE</span><span class="x">(),</span> <span class="n">ParticleSwarm</span><span class="x">())</span>

<span class="c"># Use Newton</span>
<span class="n">mle_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MLE</span><span class="x">(),</span> <span class="n">Newton</span><span class="x">())</span>

<span class="c"># Use AcceleratedGradientDescent</span>
<span class="n">mle_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MLE</span><span class="x">(),</span> <span class="n">AcceleratedGradientDescent</span><span class="x">())</span>
</code></pre></div></div>

<p>Some methods may have trouble calculating the mode because not enough iterations were allowed, or the target function moved upwards between function calls. Turing will warn you if Optim fails to converge by running <code class="language-plaintext highlighter-rouge">Optim.converge</code>. A typical solution to this might be to add more iterations, or allow the optimizer to increase between function iterations:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Increase the iterations and allow function eval to increase between calls.</span>
<span class="n">mle_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MLE</span><span class="x">(),</span> <span class="n">Newton</span><span class="x">(),</span> <span class="n">Optim</span><span class="o">.</span><span class="n">Options</span><span class="x">(</span><span class="n">iterations</span><span class="o">=</span><span class="mi">10_000</span><span class="x">,</span> <span class="n">allow_f_increases</span><span class="o">=</span><span class="nb">true</span><span class="x">))</span>
</code></pre></div></div>

<p>More options for Optim are available <a href="https://julianlsolvers.github.io/Optim.jl/stable/#user/config/">here</a>.</p>

<p><a id="Analyzing-your-mode-estimate"></a></p>

<p><a id="Analyzing-your-mode-estimate-1"></a></p>

<h4 id="analyzing-your-mode-estimate">Analyzing your mode estimate</h4>

<p>Turing extends several methods from <code class="language-plaintext highlighter-rouge">StatsBase</code> that can be used to analyze your mode estimation results. Methods implemented include <code class="language-plaintext highlighter-rouge">vcov</code>, <code class="language-plaintext highlighter-rouge">informationmatrix</code>, <code class="language-plaintext highlighter-rouge">coeftable</code>, <code class="language-plaintext highlighter-rouge">params</code>, and <code class="language-plaintext highlighter-rouge">coef</code>, among others.</p>

<p>For example, let’s examine our ML estimate from above using <code class="language-plaintext highlighter-rouge">coeftable</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Import StatsBase to use it's statistical methods.</span>
<span class="k">using</span> <span class="n">StatsBase</span>

<span class="c"># Print out the coefficient table.</span>
<span class="n">coeftable</span><span class="x">(</span><span class="n">mle_estimate</span><span class="x">)</span>
</code></pre></div></div>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">─────────────────────────────</span>
   <span class="n">estimate</span>  <span class="n">stderror</span>   <span class="n">tstat</span>
<span class="n">─────────────────────────────</span>
<span class="n">s</span>    <span class="mf">0.0625</span>  <span class="mf">0.0625</span>    <span class="mf">1.0</span>
<span class="n">m</span>    <span class="mf">1.75</span>    <span class="mf">0.176777</span>  <span class="mf">9.8995</span>
<span class="n">─────────────────────────────</span>
</code></pre></div></div>

<p>Standard errors are calculated from the Fisher information matrix (inverse Hessian of the log likelihood or log joint). t-statistics will be familiar to frequentist statisticians. Warning – standard errors calculated in this way may not always be appropriate for MAP estimates, so please be cautious in interpreting them.</p>

<p><a id="Sampling-with-the-MAP/MLE-as-initial-states"></a></p>

<p><a id="Sampling-with-the-MAP/MLE-as-initial-states-1"></a></p>

<h4 id="sampling-with-the-mapmle-as-initial-states">Sampling with the MAP/MLE as initial states</h4>

<p>You can begin sampling your chain from an MLE/MAP estimate by extracting the vector of parameter values and providing it to the <code class="language-plaintext highlighter-rouge">sample</code> function with the keyword <code class="language-plaintext highlighter-rouge">init_theta</code>. For example, here is how to sample from the full posterior using the MAP estimate as the starting point:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Generate an MAP estimate.</span>
<span class="n">map_estimate</span> <span class="o">=</span> <span class="n">optimize</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">MAP</span><span class="x">())</span>

<span class="c"># Sample with the MAP estimate as the starting point.</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">NUTS</span><span class="x">(),</span> <span class="mi">1_000</span><span class="x">,</span> <span class="n">init_theta</span> <span class="o">=</span> <span class="n">map_estimate</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">array</span><span class="x">)</span>
</code></pre></div></div>

<p><a id="Beyond-the-Basics"></a></p>

<p><a id="Beyond-the-Basics-1"></a></p>

<h2 id="beyond-the-basics">Beyond the Basics</h2>

<p><a id="Compositional-Sampling-Using-Gibbs"></a></p>

<p><a id="Compositional-Sampling-Using-Gibbs-1"></a></p>

<h3 id="compositional-sampling-using-gibbs">Compositional Sampling Using Gibbs</h3>

<p>Turing.jl provides a Gibbs interface to combine different samplers. For example, one can combine an <code class="language-plaintext highlighter-rouge">HMC</code> sampler with a <code class="language-plaintext highlighter-rouge">PG</code> sampler to run inference for different parameters in a single model as below.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="k">function</span><span class="nf"> simple_choice</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
    <span class="n">p</span> <span class="o">~</span> <span class="n">Beta</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">2</span><span class="x">)</span>
    <span class="n">z</span> <span class="o">~</span> <span class="n">Bernoulli</span><span class="x">(</span><span class="n">p</span><span class="x">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="x">(</span><span class="n">xs</span><span class="x">)</span>
        <span class="k">if</span> <span class="n">z</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="n">xs</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
        <span class="k">else</span>
            <span class="n">xs</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
        <span class="k">end</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="n">simple_choice_f</span> <span class="o">=</span> <span class="n">simple_choice</span><span class="x">([</span><span class="mf">1.5</span><span class="x">,</span> <span class="mf">2.0</span><span class="x">,</span> <span class="mf">0.3</span><span class="x">])</span>

<span class="n">chn</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">simple_choice_f</span><span class="x">,</span> <span class="n">Gibbs</span><span class="x">(</span><span class="n">HMC</span><span class="x">(</span><span class="mf">0.2</span><span class="x">,</span> <span class="mi">3</span><span class="x">,</span> <span class="o">:</span><span class="n">p</span><span class="x">),</span> <span class="n">PG</span><span class="x">(</span><span class="mi">20</span><span class="x">,</span> <span class="o">:</span><span class="n">z</span><span class="x">)),</span> <span class="mi">1000</span><span class="x">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">Gibbs</code> sampler can be used to specify unique automatic differentation backends for different variable spaces. Please see the <a href="/v0.14/docs/using-turing/autodiff">Automatic Differentiation</a> article for more.</p>

<p>For more details of compositional sampling in Turing.jl, please check the corresponding <a href="http://proceedings.mlr.press/v84/ge18b.html">paper</a>.</p>

<p><a id="Working-with-MCMCChains.jl"></a></p>

<p><a id="Working-with-MCMCChains.jl-1"></a></p>

<h3 id="working-with-mcmcchainsjl">Working with MCMCChains.jl</h3>

<p>Turing.jl wraps its samples using <code class="language-plaintext highlighter-rouge">MCMCChains.Chain</code> so that all the functions working for <code class="language-plaintext highlighter-rouge">MCMCChains.Chain</code> can be re-used in Turing.jl. Two typical functions are <code class="language-plaintext highlighter-rouge">MCMCChains.describe</code> and <code class="language-plaintext highlighter-rouge">MCMCChains.plot</code>, which can be used as follows for an obtained chain <code class="language-plaintext highlighter-rouge">chn</code>. For more information on <code class="language-plaintext highlighter-rouge">MCMCChains</code>, please see the <a href="https://github.com/TuringLang/MCMCChains.jl">GitHub repository</a>.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">describe</span><span class="x">(</span><span class="n">chn</span><span class="x">)</span> <span class="c"># Lists statistics of the samples.</span>
<span class="n">plot</span><span class="x">(</span><span class="n">chn</span><span class="x">)</span> <span class="c"># Plots statistics of the samples.</span>
</code></pre></div></div>

<p>There are numerous functions in addition to <code class="language-plaintext highlighter-rouge">describe</code> and <code class="language-plaintext highlighter-rouge">plot</code> in the <code class="language-plaintext highlighter-rouge">MCMCChains</code> package, such as those used in convergence diagnostics. For more information on the package, please see the <a href="https://github.com/TuringLang/MCMCChains.jl">GitHub repository</a>.</p>

<p><a id="Working-with-Libtask.jl"></a></p>

<p><a id="Working-with-Libtask.jl-1"></a></p>

<h3 id="working-with-libtaskjl">Working with Libtask.jl</h3>

<p>The <a href="https://github.com/TuringLang/Libtask.jl">Libtask.jl</a> library provides write-on-copy data structures that are safe for use in Turing’s particle-based samplers. One data structure in particular is often required for use – the <a href="http://turing.ml/docs/library/#Libtask.TArray"><code class="language-plaintext highlighter-rouge">TArray</code></a>. The following sampler types require the use of a <code class="language-plaintext highlighter-rouge">TArray</code> to store distributions:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">IPMCMC</code></li>
  <li><code class="language-plaintext highlighter-rouge">IS</code></li>
  <li><code class="language-plaintext highlighter-rouge">PG</code></li>
  <li><code class="language-plaintext highlighter-rouge">PMMH</code></li>
  <li><code class="language-plaintext highlighter-rouge">SMC</code></li>
</ul>

<p>If you do not use a <code class="language-plaintext highlighter-rouge">TArray</code> to store arrays of distributions when using a particle-based sampler, you may experience errors.</p>

<p>Here is an example of how the <code class="language-plaintext highlighter-rouge">TArray</code> (using a <code class="language-plaintext highlighter-rouge">TArray</code> constructor function called <code class="language-plaintext highlighter-rouge">tzeros</code>) can be applied in this way:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Turing model definition.</span>
<span class="nd">@model</span> <span class="k">function</span><span class="nf"> BayesHmm</span><span class="x">(</span><span class="n">y</span><span class="x">)</span>
    <span class="c"># Declare a TArray with a length of N.</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">tzeros</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span> <span class="n">N</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Real</span><span class="x">}(</span><span class="nb">undef</span><span class="x">,</span> <span class="n">K</span><span class="x">)</span>
    <span class="n">T</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Vector</span><span class="x">{</span><span class="kt">Real</span><span class="x">}}(</span><span class="nb">undef</span><span class="x">,</span> <span class="n">K</span><span class="x">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">K</span>
        <span class="n">T</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Dirichlet</span><span class="x">(</span><span class="n">ones</span><span class="x">(</span><span class="n">K</span><span class="x">)</span><span class="o">/</span><span class="n">K</span><span class="x">)</span>
        <span class="n">m</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">i</span><span class="x">,</span> <span class="mf">0.01</span><span class="x">)</span>
    <span class="k">end</span>

    <span class="c"># Draw from a distribution for each element in s.</span>
    <span class="n">s</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span> <span class="o">~</span> <span class="n">Categorical</span><span class="x">(</span><span class="n">K</span><span class="x">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">2</span><span class="o">:</span><span class="n">N</span>
        <span class="n">s</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Categorical</span><span class="x">(</span><span class="n">vec</span><span class="x">(</span><span class="n">T</span><span class="x">[</span><span class="n">s</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">]]))</span>
        <span class="n">y</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">[</span><span class="n">s</span><span class="x">[</span><span class="n">i</span><span class="x">]],</span> <span class="mf">0.1</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="x">(</span><span class="n">s</span><span class="x">,</span> <span class="n">m</span><span class="x">)</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<p><a id="Changing-Default-Settings"></a></p>

<p><a id="Changing-Default-Settings-1"></a></p>

<h3 id="changing-default-settings">Changing Default Settings</h3>

<p>Some of Turing.jl’s default settings can be changed for better usage.</p>

<p><a id="AD-Chunk-Size"></a></p>

<p><a id="AD-Chunk-Size-1"></a></p>

<h4 id="ad-chunk-size">AD Chunk Size</h4>

<p>ForwardDiff (Turing’s default AD backend) uses forward-mode chunk-wise AD. The chunk size can be manually set by <code class="language-plaintext highlighter-rouge">setchunksize(new_chunk_size)</code>; alternatively, use an auto-tuning helper function <code class="language-plaintext highlighter-rouge">auto_tune_chunk_size!(mf::Function, rep_num=10)</code>, which will profile various chunk sizes. Here <code class="language-plaintext highlighter-rouge">mf</code> is the model function, e.g. <code class="language-plaintext highlighter-rouge">gdemo(1.5, 2)</code>, and <code class="language-plaintext highlighter-rouge">rep_num</code> is the number of repetitions during profiling.</p>

<p><a id="AD-Backend"></a></p>

<p><a id="AD-Backend-1"></a></p>

<h4 id="ad-backend">AD Backend</h4>

<p>Turing supports four packages of automatic differentiation (AD) in the back end during sampling. The default AD backend is <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff</a> for forward-mode AD. Three reverse-mode AD backends are also supported, namely <a href="https://github.com/FluxML/Tracker.jl">Tracker</a>, <a href="https://github.com/FluxML/Zygote.jl">Zygote</a> and <a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff</a>. <code class="language-plaintext highlighter-rouge">Zygote</code> and <code class="language-plaintext highlighter-rouge">ReverseDiff</code> are supported optionally if explicitly loaded by the user with <code class="language-plaintext highlighter-rouge">using Zygote</code> or <code class="language-plaintext highlighter-rouge">using ReverseDiff</code> next to <code class="language-plaintext highlighter-rouge">using Turing</code>.</p>

<p>For more information on Turing’s automatic differentiation backend, please see the <a href="/v0.14/docs/using-turing/autodiff">Automatic Differentiation</a> article.</p>

<p><a id="Progress-Logging"></a></p>

<p><a id="Progress-Logging-1"></a></p>

<h4 id="progress-logging">Progress Logging</h4>

<p>Turing.jl uses ProgressLogging.jl to log the progress of sampling. Progress logging is enabled as default but might slow down inference. It can be turned on or off by setting the keyword argument <code class="language-plaintext highlighter-rouge">progress</code> of <code class="language-plaintext highlighter-rouge">sample</code> to <code class="language-plaintext highlighter-rouge">true</code> or <code class="language-plaintext highlighter-rouge">false</code>, respectively. Moreover, you can enable or disable progress logging globally by calling <code class="language-plaintext highlighter-rouge">setprogress!(true)</code> or <code class="language-plaintext highlighter-rouge">setprogress!(false)</code>, respectively.</p>

<p>Turing uses heuristics to select an appropriate visualization backend. If you use <a href="https://junolab.org/">Juno</a>, the progress is displayed with a <a href="http://docs.junolab.org/latest/man/juno_frontend/#Progress-Meters-1">progress bar in the Atom window</a>. For Jupyter notebooks the default backend is <a href="https://github.com/tkf/ConsoleProgressMonitor.jl">ConsoleProgressMonitor.jl</a>. In all other cases progress logs are displayed with <a href="https://github.com/c42f/TerminalLoggers.jl">TerminalLoggers.jl</a>. Alternatively, if you provide a custom visualization backend, Turing uses it instead of the default backend.</p>

