<p><a id="How-Turing-implements-AbstractMCMC"></a></p>

<p><a id="How-Turing-implements-AbstractMCMC-1"></a></p>

<h1 id="how-turing-implements-abstractmcmc">How Turing implements AbstractMCMC</h1>

<p>Prerequisite: <a href="https://turing.ml/dev/docs/for-developers/interface">Interface guide</a>.</p>

<p><a id="Introduction"></a></p>

<p><a id="Introduction-1"></a></p>

<h2 id="introduction">Introduction</h2>

<p>Consider the following Turing, code block:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@model</span> <span class="k">function</span><span class="nf"> gdemo</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span>
    <span class="n">s</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
    <span class="n">m</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">x</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
    <span class="n">y</span> <span class="o">~</span> <span class="n">Normal</span><span class="x">(</span><span class="n">m</span><span class="x">,</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">s</span><span class="x">))</span>
<span class="k">end</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">)</span>
<span class="n">alg</span> <span class="o">=</span> <span class="n">IS</span><span class="x">()</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">chn</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">mod</span><span class="x">,</span> <span class="n">alg</span><span class="x">,</span> <span class="n">n_samples</span><span class="x">)</span>
</code></pre></div></div>

<p>The function <code class="language-plaintext highlighter-rouge">sample</code> is part of the AbstractMCMC interface. As explained in the <a href="https://turing.ml/dev/docs/for-developers/interface">interface guide</a>, building a a sampling method that can be used by <code class="language-plaintext highlighter-rouge">sample</code> consists in overloading the structs and functions in <code class="language-plaintext highlighter-rouge">AbstractMCMC</code>. The interface guide also gives a standalone example of their implementation, <a href=""><code class="language-plaintext highlighter-rouge">AdvancedMH.jl</code></a>.</p>

<p>Turing sampling methods (most of which are written <a href="https://github.com/TuringLang/Turing.jl/tree/master/src/inference">here</a>) also implement <code class="language-plaintext highlighter-rouge">AbstractMCMC</code>. Turing defines a particular architecture for <code class="language-plaintext highlighter-rouge">AbstractMCMC</code> implementations, that enables working with models defined by the <code class="language-plaintext highlighter-rouge">@model</code> macro, and uses DynamicPPL as a backend. The goal of this page is to describe this architecture, and how you would go about implementing your own sampling method in Turing, using Importance Sampling as an example. I don’t go into all the details: for instance, I don’t address selectors or parallelism.</p>

<p>First, we explain how Importance Sampling works in the abstract. Consider the model defined in the first code block. Mathematically, it can be written:</p>

\[\begin{align}
s &amp;\sim \text{InverseGamma}(2, 3) \\
m &amp;\sim \text{Normal}(0, \sqrt{s}) \\
x &amp;\sim \text{Normal}(m, \sqrt{s}) \\
y &amp;\sim \text{Normal}(m, \sqrt{s})
\end{align}\]

<p>The <strong>latent</strong> variables are \(s\) and \(m\), the <strong>observed</strong> variables are \(x\) and \(y\). The model <strong>joint</strong> distribution \(p(s,m,x,y)\) decomposes into the <strong>prior</strong> \(p(s,m)\) and the <strong>likelihood</strong> \(p(x,y \mid s,m)\). Since \(x = 1.5\) and \(y = 2\) are observed, the goal is to infer the <strong>posterior</strong> distribution \(p(s,m \mid x,y)\).</p>

<p>Importance Sampling produces independent samples \((s_i, m_i)\) from the prior distribution. It also outputs unnormalized weights \(w_i = \frac {p(x,y,s_i,m_i)} {p(s_i, m_i)} = p(x,y \mid s_i, m_i)\) such that the empirical distribution \(\frac 1 N \sum\limits_{i =1}^N \frac {w*i} {\sum\limits*{j=1}^N w_j} \delta*{(s\*i, m_i)}\) is a good approximation of the posterior.</p>

<p><a id=".-Define-a-Sampler"></a></p>

<p><a id=".-Define-a-Sampler-1"></a></p>

<h2 id="1-define-a-sampler">1. Define a <code class="language-plaintext highlighter-rouge">Sampler</code></h2>

<p>Recall the last line of the above code block:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chn</span> <span class="o">=</span> <span class="n">sample</span><span class="x">(</span><span class="n">mod</span><span class="x">,</span> <span class="n">alg</span><span class="x">,</span> <span class="n">n_samples</span><span class="x">)</span>
</code></pre></div></div>

<p>Here <code class="language-plaintext highlighter-rouge">sample</code> takes as arguments a <strong>model</strong> <code class="language-plaintext highlighter-rouge">mod</code>, an <strong>algorithm</strong> <code class="language-plaintext highlighter-rouge">alg</code>, and a <strong>number of samples</strong> <code class="language-plaintext highlighter-rouge">n_samples</code>, and returns an instance <code class="language-plaintext highlighter-rouge">chn</code> of <code class="language-plaintext highlighter-rouge">Chains</code> which can be analysed using the functions in <code class="language-plaintext highlighter-rouge">MCMCChains</code>.</p>

<p><a id="Models"></a></p>

<p><a id="Models-1"></a></p>

<h3 id="models">Models</h3>

<p>To define a <strong>model</strong>, you declare a joint distribution on variables in the <code class="language-plaintext highlighter-rouge">@model</code> macro, and specify which variables are observed and which should be inferred, as well as the value of the observed variables. Thus, when implementing Importance Sampling,</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mod</span> <span class="o">=</span> <span class="n">gdemo</span><span class="x">(</span><span class="mf">1.5</span><span class="x">,</span> <span class="mi">2</span><span class="x">)</span>
</code></pre></div></div>

<p>creates an instance <code class="language-plaintext highlighter-rouge">mod</code> of the struct <code class="language-plaintext highlighter-rouge">Model</code>, which corresponds to the observations of a value of <code class="language-plaintext highlighter-rouge">1.5</code> for <code class="language-plaintext highlighter-rouge">x</code>, and a value of <code class="language-plaintext highlighter-rouge">2</code> for <code class="language-plaintext highlighter-rouge">y</code>.</p>

<p>This is all handled by DynamicPPL, more specifically <a href="https://github.com/TuringLang/DynamicPPL.jl/blob/master/src/model.jl">here</a>. I will return to how models are used to inform sampling algorithms <a href="#assumeobserve">below</a>.</p>

<p><a id="Algorithms"></a></p>

<p><a id="Algorithms-1"></a></p>

<h3 id="algorithms">Algorithms</h3>

<p>An <strong>algorithm</strong> is just a sampling method: in Turing, it is a subtype of the abstract type <code class="language-plaintext highlighter-rouge">InferenceAlgorithm</code>. Defining an algorithm may require specifying a few high-level parameters. For example, “Hamiltonian Monte-Carlo” may be too vague, but “Hamiltonian Monte Carlo with  10 leapfrog steps per proposal and a stepsize of 0.01” is an algorithm. “Metropolis-Hastings” may be too vague, but “Metropolis-Hastings with proposal distribution <code class="language-plaintext highlighter-rouge">p</code>” is an algorithm. \(\epsilon\)</p>

<p>Thus</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stepsize</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">L</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">alg</span> <span class="o">=</span> <span class="n">HMC</span><span class="x">(</span><span class="n">stepsize</span><span class="x">,</span> <span class="n">L</span><span class="x">)</span>
</code></pre></div></div>

<p>defines a Hamiltonian Monte-Carlo algorithm, an instance of <code class="language-plaintext highlighter-rouge">HMC</code>, which is a subtype of <code class="language-plaintext highlighter-rouge">InferenceAlgorithm</code>.</p>

<p>In the case of Importance Sampling, there is no need to specify additional parameters:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alg</span> <span class="o">=</span> <span class="n">IS</span><span class="x">()</span>
</code></pre></div></div>

<p>defines an Importance Sampling algorithm, an instance of <code class="language-plaintext highlighter-rouge">IS</code> which is a subtype of <code class="language-plaintext highlighter-rouge">InferenceAlgorithm</code>.</p>

<p>When creating your own Turing sampling method, you must therefore build a subtype of <code class="language-plaintext highlighter-rouge">InferenceAlgorithm</code> corresponding to your method.</p>

<p><a id="Samplers"></a></p>

<p><a id="Samplers-1"></a></p>

<h3 id="samplers">Samplers</h3>

<p>Samplers are <strong>not</strong> the same as algorithms. An algorithm is a generic sampling method, a sampler is an object that stores information about how algorithm and model interact during sampling, and is modified as sampling progresses. The <code class="language-plaintext highlighter-rouge">Sampler</code> struct is defined in DynamicPPL.</p>

<p>Turing implements <code class="language-plaintext highlighter-rouge">AbstractMCMC</code>’s <code class="language-plaintext highlighter-rouge">AbstractSampler</code> with the <code class="language-plaintext highlighter-rouge">Sampler</code> struct defined in <code class="language-plaintext highlighter-rouge">DynamicPPL</code>. The most important attributes of an instance <code class="language-plaintext highlighter-rouge">spl</code> of <code class="language-plaintext highlighter-rouge">Sampler</code> are:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">spl.alg</code>: the sampling method used, an instance of a subtype of <code class="language-plaintext highlighter-rouge">InferenceAlgorithm</code></li>
  <li><code class="language-plaintext highlighter-rouge">spl.state</code>: information about the sampling process, see <a href="#States">below</a></li>
</ul>

<p>When you call <code class="language-plaintext highlighter-rouge">sample(mod, alg, n_samples)</code>, Turing first uses <code class="language-plaintext highlighter-rouge">model</code> and <code class="language-plaintext highlighter-rouge">alg</code> to build an instance <code class="language-plaintext highlighter-rouge">spl</code> of <code class="language-plaintext highlighter-rouge">Sampler</code> , then calls the native <code class="language-plaintext highlighter-rouge">AbstractMCMC</code> function <code class="language-plaintext highlighter-rouge">sample(mod, spl, n_samples)</code>.</p>

<p>When you define your own Turing sampling method, you must therefore build:</p>

<ul>
  <li>a <strong>sampler constructor</strong> that uses a model and an algorithm to initialize an instance of <code class="language-plaintext highlighter-rouge">Sampler</code>. For Importance Sampling:</li>
</ul>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> Sampler</span><span class="x">(</span><span class="n">alg</span><span class="o">::</span><span class="n">IS</span><span class="x">,</span> <span class="n">model</span><span class="o">::</span><span class="n">Model</span><span class="x">,</span> <span class="n">s</span><span class="o">::</span><span class="n">Selector</span><span class="x">)</span>
    <span class="n">info</span> <span class="o">=</span> <span class="kt">Dict</span><span class="x">{</span><span class="kt">Symbol</span><span class="x">,</span> <span class="kt">Any</span><span class="x">}()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">ISState</span><span class="x">(</span><span class="n">model</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">Sampler</span><span class="x">(</span><span class="n">alg</span><span class="x">,</span> <span class="n">info</span><span class="x">,</span> <span class="n">s</span><span class="x">,</span> <span class="n">state</span><span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>

<ul>
  <li>a <strong>state</strong> struct implementing <code class="language-plaintext highlighter-rouge">AbstractSamplerState</code> corresponding to your method: we cover this in the following paragraph.</li>
</ul>

<p><a id="a-style&quot;text-decoration:none&quot;-name&quot;States&quot;States/a"></a></p>

<p><a id="a-style&quot;text-decoration:none&quot;-name&quot;States&quot;States/a-1"></a></p>

<h3 id="states"><a style="text-decoration:none" name="States">States</a></h3>

<p>The <code class="language-plaintext highlighter-rouge">vi</code> field contains all the important information about sampling: first and foremost, the values of all the samples, but also the distributions from which they are sampled, the names of model parameters, and other metadata. As we will see below, many important steps during sampling correspond to queries or updates to <code class="language-plaintext highlighter-rouge">spl.state.vi</code>.</p>

<p>By default, you can use <code class="language-plaintext highlighter-rouge">SamplerState</code>, a concrete type defined in <code class="language-plaintext highlighter-rouge">inference/Inference.jl</code>, which extends <code class="language-plaintext highlighter-rouge">AbstractSamplerState</code> and has no field except for <code class="language-plaintext highlighter-rouge">vi</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">mutable struct</span><span class="nc"> SamplerState</span><span class="x">{</span><span class="n">VIType</span><span class="o">&lt;:</span><span class="n">VarInfo</span><span class="x">}</span> <span class="o">&lt;:</span> <span class="n">AbstractSamplerState</span>
    <span class="n">vi</span> <span class="o">::</span> <span class="n">VIType</span>
<span class="k">end</span>
</code></pre></div></div>

<p>When doing Importance Sampling, we care not only about the values of the samples but also their weights. We will see below that the weight of each sample is also added to <code class="language-plaintext highlighter-rouge">spl.state.vi</code>. Moreover, the average \(\frac 1 N \sum\limits_{j=1}^N w_i = \frac 1 N \sum\limits_{j=1}^N p(x,y \mid s_i, m_i)\) of the sample weights is a particularly important quantity:</p>

<ul>
  <li>it is used to <strong>normalize</strong> the <strong>empirical approximation</strong> of the posterior distribution</li>
  <li>its logarithm is the importance sampling <strong>estimate</strong> of the <strong>log evidence</strong> \(\log p(x, y)\)</li>
</ul>

<p>To avoid having to compute it over and over again, <code class="language-plaintext highlighter-rouge">is.jl</code>defines an IS-specific concrete type <code class="language-plaintext highlighter-rouge">ISState</code> for sampler states, with an additional field <code class="language-plaintext highlighter-rouge">final_logevidence</code> containing \(\log \left( \frac 1 N \sum\limits_{j=1}^N w_i \right)\).</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">mutable struct</span><span class="nc"> ISState</span><span class="x">{</span><span class="n">V</span><span class="o">&lt;:</span><span class="n">VarInfo</span><span class="x">,</span> <span class="n">F</span><span class="o">&lt;:</span><span class="kt">AbstractFloat</span><span class="x">}</span> <span class="o">&lt;:</span> <span class="n">AbstractSamplerState</span>
    <span class="n">vi</span>                 <span class="o">::</span>  <span class="n">V</span>
    <span class="n">final_logevidence</span>  <span class="o">::</span>  <span class="n">F</span>
<span class="k">end</span>

<span class="c"># additional constructor</span>
<span class="n">ISState</span><span class="x">(</span><span class="n">model</span><span class="o">::</span><span class="n">Model</span><span class="x">)</span> <span class="o">=</span> <span class="n">ISState</span><span class="x">(</span><span class="n">VarInfo</span><span class="x">(</span><span class="n">model</span><span class="x">),</span> <span class="mf">0.0</span><span class="x">)</span>
</code></pre></div></div>

<p>The following diagram summarizes the hierarchy presented above.</p>

<p><img src="how_turing_implements_abstractmcmc_files/hierarchy.png" alt="hierarchy" /></p>

<p><a id=".-Overload-the-functions-used-inside-mcmcsample"></a></p>

<p><a id=".-Overload-the-functions-used-inside-mcmcsample-1"></a></p>

<h2 id="2-overload-the-functions-used-inside-mcmcsample">2. Overload the functions used inside <code class="language-plaintext highlighter-rouge">mcmcsample</code></h2>

<p>A lot of the things here are method-specific. However Turing also has some functions that make it easier for you to implement these functions, for examples .</p>

<p><a id="Transitions"></a></p>

<p><a id="Transitions-1"></a></p>

<h3 id="transitions">Transitions</h3>

<p><code class="language-plaintext highlighter-rouge">AbstractMCMC</code> stores information corresponding to each individual sample in objects called <code class="language-plaintext highlighter-rouge">transition</code>, but does not specify what the structure of these objects could be. You could decide to implement a type <code class="language-plaintext highlighter-rouge">MyTransition</code> for transitions corresponding to the specifics of your methods. However, there are many situations in which the only information you need for each sample is:</p>

<ul>
  <li>its value: \(\theta\)</li>
  <li>log of the joint probability of the observed data and this sample: <code class="language-plaintext highlighter-rouge">lp</code></li>
</ul>

<p><code class="language-plaintext highlighter-rouge">Inference.jl</code> <a href="https://github.com/TuringLang/Turing.jl/blob/master/src/inference/Inference.jl#L103">defines</a> a struct <code class="language-plaintext highlighter-rouge">Transition</code>, which corresponds to this default situation</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span><span class="nc"> Transition</span><span class="x">{</span><span class="n">T</span><span class="x">,</span> <span class="n">F</span><span class="o">&lt;:</span><span class="kt">AbstractFloat</span><span class="x">}</span>
    <span class="n">θ</span>  <span class="o">::</span> <span class="n">T</span>
    <span class="n">lp</span> <span class="o">::</span> <span class="n">F</span>
<span class="k">end</span>
</code></pre></div></div>

<p>It also <a href="https://github.com/TuringLang/Turing.jl/blob/master/src/inference/Inference.jl#L108">contains</a> a constructor that builds an instance of <code class="language-plaintext highlighter-rouge">Transition</code> from an instance <code class="language-plaintext highlighter-rouge">spl</code> of <code class="language-plaintext highlighter-rouge">Sampler</code>: \(\theta\) is <code class="language-plaintext highlighter-rouge">spl.state.vi</code> converted to a <code class="language-plaintext highlighter-rouge">namedtuple</code>, and <code class="language-plaintext highlighter-rouge">lp</code> is <code class="language-plaintext highlighter-rouge">getlogp(spl.state.vi)</code>. <code class="language-plaintext highlighter-rouge">is.jl</code> uses this default constructor at the end of the <code class="language-plaintext highlighter-rouge">step!</code> function <a href="https://github.com/TuringLang/Turing.jl/blob/master/src/inference/is.jl#L58">here</a>.</p>

<p><a id="How-sample-works"></a></p>

<p><a id="How-sample-works-1"></a></p>

<h3 id="how-sample-works">How <code class="language-plaintext highlighter-rouge">sample</code> works</h3>

<p>A crude summary, which ignores things like parallelism, is the following:</p>

<p><code class="language-plaintext highlighter-rouge">sample</code> calls <code class="language-plaintext highlighter-rouge">mcmcsample</code>, which calls</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">sample_init!</code> to set things up</li>
  <li><code class="language-plaintext highlighter-rouge">step!</code> repeatedly to produce multiple new transitions</li>
  <li><code class="language-plaintext highlighter-rouge">sample_end!</code> to perform operations once all samples have been obtained</li>
  <li><code class="language-plaintext highlighter-rouge">bundle_samples</code> to convert a vector of transitions into a more palatable type, for instance a <code class="language-plaintext highlighter-rouge">Chain</code>.</li>
</ul>

<p>You can of course implement all of these functions, but <code class="language-plaintext highlighter-rouge">AbstractMCMC</code> as well as Turing also provide default implementations for simple cases. For instance, importance sampling uses the default implementations of <code class="language-plaintext highlighter-rouge">sample_init!</code> and <code class="language-plaintext highlighter-rouge">bundle_samples</code>, which is why you don’t see code for them inside <code class="language-plaintext highlighter-rouge">is.jl</code>.</p>

<p><a id="a-style&quot;text-decoration:none&quot;-name&quot;assumeobserve&quot;3.-Overload-assume-and-observe/a"></a></p>

<p><a id="a-style&quot;text-decoration:none&quot;-name&quot;assumeobserve&quot;3.-Overload-assume-and-observe/a-1"></a></p>

<h2 id="3-overload-assume-and-observe"><a style="text-decoration:none" name="assumeobserve">3. Overload <code class="language-plaintext highlighter-rouge">assume</code> and <code class="language-plaintext highlighter-rouge">observe</code></a></h2>

<p>The functions mentioned above, such as <code class="language-plaintext highlighter-rouge">sample_init!</code>, <code class="language-plaintext highlighter-rouge">step!</code>, etc.,  must of course use information about the model in order to generate samples! In particular, these functions may need <strong>samples from distributions</strong> defined in the model, or to <strong>evaluate the density of these distributions</strong> at some values of the corresponding parameters or observations.</p>

<p>For an example of the former, consider <strong>Importance Sampling</strong> as defined in <code class="language-plaintext highlighter-rouge">is.jl</code>. This implementation of Importance Sampling uses the model prior distribution as a proposal distribution, and therefore requires <strong>samples from the prior distribution</strong> of the model. Another example is <strong>Approximate Bayesian Computation</strong>, which requires multiple <strong>samples from the model prior and likelihood distributions</strong> in order to generate a single sample.</p>

<p>An example of the latter is the <strong>Metropolis-Hastings</strong> algorithm. At every step of sampling from a target posterior \(p(\theta \mid x_{\text{obs}})\), in order to compute the acceptance ratio, you need to <strong>evaluate the model joint density</strong> \(p(\theta_{\text{prop}}, x_{\text{obs}})\) with \(\theta_{\text{prop}}\) a sample from the proposal and \(x_{\text{obs}}\) the observed data.</p>

<p>This begs the question: how can these functions access model information during sampling? Recall that the model is stored as an instance <code class="language-plaintext highlighter-rouge">m</code> of <code class="language-plaintext highlighter-rouge">Model</code>. One of the attributes of <code class="language-plaintext highlighter-rouge">m</code> is the model evaluation function <code class="language-plaintext highlighter-rouge">m.f</code>, which is built by compiling the <code class="language-plaintext highlighter-rouge">@model</code> macro. Executing <code class="language-plaintext highlighter-rouge">f</code> runs the tilde statements of the model in order, and adds model information to the sampler (the instance of <code class="language-plaintext highlighter-rouge">Sampler</code> that stores information about the ongoing sampling process) at each step (see <a href="https://turing.ml/dev/docs/for-developers/compiler">here</a> for more information about how the <code class="language-plaintext highlighter-rouge">@model</code> macro is compiled). The DynamicPPL functions <code class="language-plaintext highlighter-rouge">assume</code> and <code class="language-plaintext highlighter-rouge">observe</code> determine what kind of information to add to the sampler for every tilde statement.</p>

<p>Consider an instance <code class="language-plaintext highlighter-rouge">m</code> of <code class="language-plaintext highlighter-rouge">Model</code> and a sampler <code class="language-plaintext highlighter-rouge">spl</code>, with associated <code class="language-plaintext highlighter-rouge">VarInfo</code> <code class="language-plaintext highlighter-rouge">vi = spl.state.vi</code>. At some point during the sampling process, an AbstractMCMC function such as <code class="language-plaintext highlighter-rouge">step!</code> calls  <code class="language-plaintext highlighter-rouge">m(vi, ...)</code>, which calls the model evaluation function <code class="language-plaintext highlighter-rouge">m.f(vi, ...)</code>.</p>

<ul>
  <li>
    <p>for every tilde statement in the <code class="language-plaintext highlighter-rouge">@model</code> macro, <code class="language-plaintext highlighter-rouge">m.f(vi, ...)</code> returns model-related information (samples, value of the model density, etc.), and adds it to <code class="language-plaintext highlighter-rouge">vi</code>. How does it do that?</p>

    <ul>
      <li>recall that the code for <code class="language-plaintext highlighter-rouge">m.f(vi, ...)</code> is automatically generated by compilation of the <code class="language-plaintext highlighter-rouge">@model</code> macro</li>
      <li>for every tilde statement in the <code class="language-plaintext highlighter-rouge">@model</code> declaration, this code contains a call to <code class="language-plaintext highlighter-rouge">assume(vi, ...)</code> if the variable on the LHS of the tilde is a <strong>model parameter to infer</strong>, and <code class="language-plaintext highlighter-rouge">observe(vi, ...)</code> if the variable on the LHS of the tilde is an <strong>observation</strong></li>
      <li>in the file corresponding to your sampling method (ie in <code class="language-plaintext highlighter-rouge">Turing.jl/src/inference/&lt;your_method&gt;.jl</code>), you have <strong>overloaded</strong> <code class="language-plaintext highlighter-rouge">assume</code> and <code class="language-plaintext highlighter-rouge">observe</code>, so that they can modify <code class="language-plaintext highlighter-rouge">vi</code> to include the information and samples that you care about!</li>
      <li>at a minimum, <code class="language-plaintext highlighter-rouge">assume</code> and <code class="language-plaintext highlighter-rouge">observe</code> return the log density <code class="language-plaintext highlighter-rouge">lp</code> of the sample or observation. the model evaluation function then immediately calls <code class="language-plaintext highlighter-rouge">acclogp!(vi, lp)</code>, which adds <code class="language-plaintext highlighter-rouge">lp</code> to the value of the log joint density stored in <code class="language-plaintext highlighter-rouge">vi</code>.</li>
    </ul>
  </li>
</ul>

<p>Here’s what <code class="language-plaintext highlighter-rouge">assume</code> looks like for Importance Sampling:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> DynamicPPL</span><span class="o">.</span><span class="n">assume</span><span class="x">(</span><span class="n">rng</span><span class="x">,</span> <span class="n">spl</span><span class="o">::</span><span class="n">Sampler</span><span class="x">{</span><span class="o">&lt;:</span><span class="n">IS</span><span class="x">},</span> <span class="n">dist</span><span class="o">::</span><span class="n">Distribution</span><span class="x">,</span> <span class="n">vn</span><span class="o">::</span><span class="n">VarName</span><span class="x">,</span> <span class="n">vi</span><span class="x">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">rng</span><span class="x">,</span> <span class="n">dist</span><span class="x">)</span>
    <span class="n">push!</span><span class="x">(</span><span class="n">vi</span><span class="x">,</span> <span class="n">vn</span><span class="x">,</span> <span class="n">r</span><span class="x">,</span> <span class="n">dist</span><span class="x">,</span> <span class="n">spl</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">r</span><span class="x">,</span> <span class="mi">0</span>
<span class="k">end</span>
</code></pre></div></div>

<p>The function first generates a sample <code class="language-plaintext highlighter-rouge">r</code> from the distribution <code class="language-plaintext highlighter-rouge">dist</code> (the right hand side of the tilde statement). It then adds <code class="language-plaintext highlighter-rouge">r</code> to <code class="language-plaintext highlighter-rouge">vi</code>, and returns <code class="language-plaintext highlighter-rouge">r</code> and 0.</p>

<p>The <code class="language-plaintext highlighter-rouge">observe</code> function is even simpler:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> DynamicPPL</span><span class="o">.</span><span class="n">observe</span><span class="x">(</span><span class="n">spl</span><span class="o">::</span><span class="n">Sampler</span><span class="x">{</span><span class="o">&lt;:</span><span class="n">IS</span><span class="x">},</span> <span class="n">dist</span><span class="o">::</span><span class="n">Distribution</span><span class="x">,</span> <span class="n">value</span><span class="x">,</span> <span class="n">vi</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">logpdf</span><span class="x">(</span><span class="n">dist</span><span class="x">,</span> <span class="n">value</span><span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p>It simply returns the density (in the discrete case, the probability) of the observed value under the distribution <code class="language-plaintext highlighter-rouge">dist</code>.</p>

<p><a id=".-Summary:-Importance-Sampling-step-by-step"></a></p>

<p><a id=".-Summary:-Importance-Sampling-step-by-step-1"></a></p>

<h2 id="4-summary-importance-sampling-step-by-step">4. Summary: Importance Sampling step by step</h2>

<p>We focus on the AbstractMCMC functions that are overriden in <code class="language-plaintext highlighter-rouge">is.jl</code> and executed inside <code class="language-plaintext highlighter-rouge">mcmcsample</code>: <code class="language-plaintext highlighter-rouge">step!</code>, which is called <code class="language-plaintext highlighter-rouge">n_samples</code> times, and <code class="language-plaintext highlighter-rouge">sample_end!</code>, which is executed once after those <code class="language-plaintext highlighter-rouge">n_samples</code> iterations.</p>

<ul>
  <li>
    <p>During the \(i\)-th iteration, <code class="language-plaintext highlighter-rouge">step!</code> does 3 things:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">empty!(spl.state.vi)</code>: remove information about the previous sample from the sampler’s <code class="language-plaintext highlighter-rouge">VarInfo</code></li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">model(rng, spl.state.vi, spl)</code>: call the model evaluation function</p>

        <ul>
          <li>calls to <code class="language-plaintext highlighter-rouge">assume</code> add the samples from the prior \(s_i\) and \(m_i\) to <code class="language-plaintext highlighter-rouge">spl.state.vi</code></li>
          <li>calls to both <code class="language-plaintext highlighter-rouge">assume</code> or <code class="language-plaintext highlighter-rouge">observe</code> are followed by the line <code class="language-plaintext highlighter-rouge">acclogp!(vi, lp)</code>, where <code class="language-plaintext highlighter-rouge">lp</code> is an output of <code class="language-plaintext highlighter-rouge">assume</code> and <code class="language-plaintext highlighter-rouge">observe</code></li>
          <li><code class="language-plaintext highlighter-rouge">lp</code> is set to 0 after <code class="language-plaintext highlighter-rouge">assume</code>, and to the value of the density at the observation after <code class="language-plaintext highlighter-rouge">observe</code></li>
          <li>when all the tilde statements have been covered, <code class="language-plaintext highlighter-rouge">spl.state.vi.logp[]</code> is the sum of the <code class="language-plaintext highlighter-rouge">lp</code>, ie the likelihood \(\log p(x, y \mid s_i, m_i) = \log p(x \mid s_i, m_i) + \log p(y \mid s_i, m_i)\) of the observations given the latent variable samples \(s_i\) and \(m_i\).</li>
        </ul>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">return Transition(spl)</code>: build a transition from the sampler, and return that transition</p>

        <ul>
          <li>the transition’s <code class="language-plaintext highlighter-rouge">vi</code> field is simply <code class="language-plaintext highlighter-rouge">spl.state.vi</code></li>
          <li>the <code class="language-plaintext highlighter-rouge">lp</code> field contains the likelihood <code class="language-plaintext highlighter-rouge">spl.state.vi.logp[]</code></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>When the, <code class="language-plaintext highlighter-rouge">n_samples</code> iterations are completed, <code class="language-plaintext highlighter-rouge">sample_end!</code> fills the <code class="language-plaintext highlighter-rouge">final_logevidence</code> field of <code class="language-plaintext highlighter-rouge">spl.state</code></p>

    <ul>
      <li>it simply takes the logarithm of the average of the sample weights, using the log weights for numerical stability</li>
    </ul>
  </li>
</ul>

